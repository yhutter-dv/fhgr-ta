{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:42:56.321135300Z",
     "start_time": "2023-11-30T07:42:42.425601400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yhutter\\AppData\\Local\\Temp\\ipykernel_30136\\3262601958.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "C:\\Users\\yhutter\\GitRepos\\fhgr-ta\\code\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\yhutter\\GitRepos\\fhgr-ta\\code\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\yhutter\\GitRepos\\fhgr-ta\\code\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils_hr as meu\n",
    "import nltk\n",
    "import textblob\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:43:27.357518600Z",
     "start_time": "2023-11-30T07:43:26.338628100Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./movie_reviews.zip', compression='zip')\n",
    "\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "sample_review_ids = [7626, 3533, 13010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:43:34.354472400Z",
     "start_time": "2023-11-30T07:43:34.283969900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "Predicted Sentiment polarity: -0.3625\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: 0.16666666666666674\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: -0.037239583333333326\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', textblob.TextBlob(review).sentiment.polarity)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:45:26.276842200Z",
     "start_time": "2023-11-30T07:45:08.009232100Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_polarity = [textblob.TextBlob(review).sentiment.polarity for review in test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Achtung: Hier wird der Cut-Point nicht bei 0 gesetzt, sondern bei 0.1\n",
    "(um bessere Ergebnisse zu erzielen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:49:56.955882800Z",
     "start_time": "2023-11-30T07:49:56.921601Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_sentiments = ['positive' if score >= 0.1 else 'negative' for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:50:02.258885500Z",
     "start_time": "2023-11-30T07:49:58.235045300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7669\n",
      "Precision: 0.767\n",
      "Recall: 0.7669\n",
      "F1 Score: 0.7668\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.78      0.77      7510\n",
      "    negative       0.77      0.76      0.76      7490\n",
      "\n",
      "    accuracy                           0.77     15000\n",
      "   macro avg       0.77      0.77      0.77     15000\n",
      "weighted avg       0.77      0.77      0.77     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       5668     1822\n",
      "        negative       1675     5835\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Sentiment Analysis with AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:50:09.403829300Z",
     "start_time": "2023-11-30T07:50:09.290217600Z"
    }
   },
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:50:12.143027800Z",
     "start_time": "2023-11-30T07:50:12.096809200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "Predicted Sentiment polarity: -7.0\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: 3.0\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: -3.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', afn.score(review))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for test dataset\n",
    "\n",
    "Achtung: Cut-Point hier ebenfalls nicht bei 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:53:28.336537500Z",
     "start_time": "2023-11-30T07:51:29.017998900Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_polarity = [afn.score(review) for review in test_reviews]\n",
    "predicted_sentiments = ['positive' if score >= 1.0 else 'negative' for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:53:29.567966600Z",
     "start_time": "2023-11-30T07:53:28.337494800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7118\n",
      "Precision: 0.7289\n",
      "Recall: 0.7118\n",
      "F1 Score: 0.7062\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.85      0.75      7510\n",
      "    negative       0.79      0.57      0.67      7490\n",
      "\n",
      "    accuracy                           0.71     15000\n",
      "   macro avg       0.73      0.71      0.71     15000\n",
      "weighted avg       0.73      0.71      0.71     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       4301     3189\n",
      "        negative       1134     6376\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:04:48.621189100Z",
     "start_time": "2023-11-30T08:04:46.328428300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Polarity Score: 0.875\n",
      "Negative Polarity Score: 0.125\n",
      "Objective Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "# ausprobieren für \"awesome\"\n",
    "awesome = list(swn.senti_synsets('awesome', 'a'))[0]\n",
    "print('Positive Polarity Score:', awesome.pos_score())\n",
    "print('Negative Polarity Score:', awesome.neg_score())\n",
    "print('Objective Score:', awesome.obj_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "hier ist es jetzt etwas komplizierter, da aus den Sentiments der Einzelwörter erst noch das Gesamt-Sentiment zusammengesetzt werden muss..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:09:42.067706900Z",
     "start_time": "2023-11-30T08:09:42.037851600Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment_sentiwordnet_lexicon(review,\n",
    "                                           verbose=False):\n",
    "\n",
    "    # tokenize and POS tag text tokens\n",
    "    tagged_text = [(token.text, token.tag_) for token in tn.nlp(review)]\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "    # get wordnet synsets based on POS tags\n",
    "    # get sentiment scores if synsets are found\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n",
    "        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n",
    "        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n",
    "        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n",
    "        # if senti-synset is found        \n",
    "        if ss_set:\n",
    "            # add scores for all found synsets\n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    # aggregate final scores\n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        # to display results in a nice table\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, \n",
    "                                         norm_neg_score, norm_final_score]],\n",
    "                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                             ['Predicted Sentiment', 'Objectivity',\n",
    "                                                              'Positive', 'Negative', 'Overall']], \n",
    "                                                             codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "        \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:09:44.165587900Z",
     "start_time": "2023-11-30T08:09:44.055064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            negative         0.7     0.11     0.19   -0.08\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive        0.76     0.19     0.06    0.13\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive         0.8     0.12     0.07    0.05\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)    \n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "(das dauert etwas länger...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:32:56.527788900Z",
     "start_time": "2023-11-30T08:11:18.474407500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roelkeheiko\\PycharmProjects\\ta_2023\\08_sentiment\\text_normalizer.py:20: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "norm_test_reviews = tn.normalize_corpus(test_reviews)\n",
    "predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in norm_test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:32:58.228973200Z",
     "start_time": "2023-11-30T08:32:56.530201700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.6323\n",
      "Precision: 0.6785\n",
      "Recall: 0.6323\n",
      "F1 Score: 0.6066\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.59      0.89      0.71      7510\n",
      "    negative       0.77      0.38      0.51      7490\n",
      "\n",
      "    accuracy                           0.63     15000\n",
      "   macro avg       0.68      0.63      0.61     15000\n",
      "weighted avg       0.68      0.63      0.61     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       2821     4669\n",
      "        negative        846     6664\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:41:45.926383100Z",
     "start_time": "2023-11-30T08:41:45.843309600Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:42:02.598429900Z",
     "start_time": "2023-11-30T08:42:02.578840600Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment_vader_lexicon(review, \n",
    "                                    threshold=0.1,\n",
    "                                    verbose=False):\n",
    "    # pre-process text\n",
    "    review = tn.strip_html_tags(review)\n",
    "    review = tn.remove_accented_chars(review)\n",
    "    review = tn.expand_contractions(review)\n",
    "    \n",
    "    # analyze the sentiment for review\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    # get aggregate scores and final sentiment\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 'positive' if agg_score >= threshold\\\n",
    "                                   else 'negative'\n",
    "    if verbose:\n",
    "        # display detailed sentiment statistics\n",
    "        positive = str(round(scores['pos'], 2)*100)+'%'\n",
    "        final = round(agg_score, 2)\n",
    "        negative = str(round(scores['neg'], 2)*100)+'%'\n",
    "        neutral = str(round(scores['neu'], 2)*100)+'%'\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive,\n",
    "                                        negative, neutral]],\n",
    "                                        columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment', 'Polarity Score',\n",
    "                                                                       'Positive', 'Negative', 'Neutral']], \n",
    "                                                              codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "    \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:42:04.086069400Z",
     "start_time": "2023-11-30T08:42:04.015669600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "     SENTIMENT STATS:                                         \n",
      "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
      "0            negative           -0.8     0.0%    40.0%   60.0%\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                                     \n",
      "  Predicted Sentiment Polarity Score Positive             Negative Neutral\n",
      "0            negative          -0.16    16.0%  14.000000000000002%   69.0%\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                         \n",
      "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
      "0            positive           0.49    11.0%    11.0%   77.0%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=True)    \n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:46:12.174436100Z",
     "start_time": "2023-11-30T08:42:09.952895300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roelkeheiko\\PycharmProjects\\ta_2023\\08_sentiment\\text_normalizer.py:20: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=False) for review in test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:47:32.711646300Z",
     "start_time": "2023-11-30T08:47:31.418344400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7109\n",
      "Precision: 0.7238\n",
      "Recall: 0.7109\n",
      "F1 Score: 0.7066\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.83      0.74      7510\n",
      "    negative       0.78      0.59      0.67      7490\n",
      "\n",
      "    accuracy                           0.71     15000\n",
      "   macro avg       0.72      0.71      0.71     15000\n",
      "weighted avg       0.72      0.71      0.71     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       4418     3072\n",
      "        negative       1265     6245\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
