{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3f8efc-cd02-408c-a8e9-b128e9a4673d",
   "metadata": {},
   "source": [
    "# Aufgabenstellung\n",
    "Gehen sie wie folgt vor:\n",
    "* Denken sie sich eigene Tests aus (Ã„hnlichkeiten, Wort-Arithmetik...)\n",
    "* Implementieren sie ein einfaches Cleaning. Probieren sie unterschiedliches Varianten aus und beurteilen sie die Ergebnisse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a3c9d3-8aa5-4ca0-8f19-0458e5e5390e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "gensim.models.doc2vec.FAST_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c051296-9c80-4664-bb66-f04a94580758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae6292-df8c-490c-a514-897997226fba",
   "metadata": {},
   "source": [
    "# Text mit Gensim einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480428ff-0557-4e98-8be7-57e33f8f15d0",
   "metadata": {},
   "source": [
    "## Text bereinigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b042045a-5a88-4f50-9b54-f9e8218ac69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "text_conent = []\n",
    "\n",
    "with open(\"./deu-ch_web-public_2019_1M-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_content = f.readlines()\n",
    "\n",
    "remove_terms = punctuation + '0123456789'\n",
    "text_content = [[character for character in sent if character not in remove_terms] for sent in text_content]\n",
    "text_content = [''.join(token) for token in text_content]\n",
    "\n",
    "with open(\"./deu-ch_web-public_2019_1M-sentences_cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(text_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc2f11b5-c7ed-435f-a5c7-0c9f521c0905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 140853 words, keeping 28536 word types\n",
      "PROGRESS: at sentence #20000, processed 281083 words, keeping 49228 word types\n",
      "PROGRESS: at sentence #30000, processed 448187 words, keeping 71594 word types\n",
      "PROGRESS: at sentence #40000, processed 602389 words, keeping 89311 word types\n",
      "PROGRESS: at sentence #50000, processed 753112 words, keeping 105352 word types\n",
      "PROGRESS: at sentence #60000, processed 896271 words, keeping 118955 word types\n",
      "PROGRESS: at sentence #70000, processed 1061214 words, keeping 133881 word types\n",
      "PROGRESS: at sentence #80000, processed 1209732 words, keeping 147891 word types\n",
      "PROGRESS: at sentence #90000, processed 1365600 words, keeping 161411 word types\n",
      "PROGRESS: at sentence #100000, processed 1518461 words, keeping 174542 word types\n",
      "PROGRESS: at sentence #110000, processed 1662897 words, keeping 186314 word types\n",
      "PROGRESS: at sentence #120000, processed 1800713 words, keeping 197937 word types\n",
      "PROGRESS: at sentence #130000, processed 1958404 words, keeping 208881 word types\n",
      "PROGRESS: at sentence #140000, processed 2112329 words, keeping 217780 word types\n",
      "PROGRESS: at sentence #150000, processed 2261002 words, keeping 227327 word types\n",
      "PROGRESS: at sentence #160000, processed 2412258 words, keeping 238209 word types\n",
      "PROGRESS: at sentence #170000, processed 2560056 words, keeping 247826 word types\n",
      "PROGRESS: at sentence #180000, processed 2700833 words, keeping 256832 word types\n",
      "PROGRESS: at sentence #190000, processed 2852305 words, keeping 266406 word types\n",
      "PROGRESS: at sentence #200000, processed 2997991 words, keeping 275675 word types\n",
      "PROGRESS: at sentence #210000, processed 3148015 words, keeping 284747 word types\n",
      "PROGRESS: at sentence #220000, processed 3301542 words, keeping 293587 word types\n",
      "PROGRESS: at sentence #230000, processed 3456527 words, keeping 303332 word types\n",
      "PROGRESS: at sentence #240000, processed 3610468 words, keeping 313108 word types\n",
      "PROGRESS: at sentence #250000, processed 3762763 words, keeping 322683 word types\n",
      "PROGRESS: at sentence #260000, processed 3915217 words, keeping 331065 word types\n",
      "PROGRESS: at sentence #270000, processed 4065857 words, keeping 339403 word types\n",
      "PROGRESS: at sentence #280000, processed 4218171 words, keeping 347976 word types\n",
      "PROGRESS: at sentence #290000, processed 4371316 words, keeping 356549 word types\n",
      "PROGRESS: at sentence #300000, processed 4521468 words, keeping 364527 word types\n",
      "PROGRESS: at sentence #310000, processed 4675743 words, keeping 373366 word types\n",
      "PROGRESS: at sentence #320000, processed 4823677 words, keeping 381016 word types\n",
      "PROGRESS: at sentence #330000, processed 4972384 words, keeping 389371 word types\n",
      "PROGRESS: at sentence #340000, processed 5120680 words, keeping 397317 word types\n",
      "PROGRESS: at sentence #350000, processed 5267495 words, keeping 404078 word types\n",
      "PROGRESS: at sentence #360000, processed 5412005 words, keeping 410692 word types\n",
      "PROGRESS: at sentence #370000, processed 5558695 words, keeping 417393 word types\n",
      "PROGRESS: at sentence #380000, processed 5708598 words, keeping 424590 word types\n",
      "PROGRESS: at sentence #390000, processed 5862021 words, keeping 431994 word types\n",
      "PROGRESS: at sentence #400000, processed 6008823 words, keeping 438315 word types\n",
      "PROGRESS: at sentence #410000, processed 6162292 words, keeping 445825 word types\n",
      "PROGRESS: at sentence #420000, processed 6312883 words, keeping 453644 word types\n",
      "PROGRESS: at sentence #430000, processed 6459335 words, keeping 460356 word types\n",
      "PROGRESS: at sentence #440000, processed 6604322 words, keeping 467434 word types\n",
      "PROGRESS: at sentence #450000, processed 6748815 words, keeping 474784 word types\n",
      "PROGRESS: at sentence #460000, processed 6894892 words, keeping 481250 word types\n",
      "PROGRESS: at sentence #470000, processed 7043327 words, keeping 486935 word types\n",
      "PROGRESS: at sentence #480000, processed 7196009 words, keeping 491873 word types\n",
      "PROGRESS: at sentence #490000, processed 7344044 words, keeping 498104 word types\n",
      "PROGRESS: at sentence #500000, processed 7481764 words, keeping 505635 word types\n",
      "PROGRESS: at sentence #510000, processed 7635727 words, keeping 512628 word types\n",
      "PROGRESS: at sentence #520000, processed 7784195 words, keeping 519545 word types\n",
      "PROGRESS: at sentence #530000, processed 7924382 words, keeping 525775 word types\n",
      "PROGRESS: at sentence #540000, processed 8059185 words, keeping 532099 word types\n",
      "PROGRESS: at sentence #550000, processed 8196617 words, keeping 538245 word types\n",
      "PROGRESS: at sentence #560000, processed 8337872 words, keeping 544234 word types\n",
      "PROGRESS: at sentence #570000, processed 8475440 words, keeping 548271 word types\n",
      "PROGRESS: at sentence #580000, processed 8627914 words, keeping 554594 word types\n",
      "PROGRESS: at sentence #590000, processed 8787658 words, keeping 561846 word types\n",
      "PROGRESS: at sentence #600000, processed 8954006 words, keeping 568777 word types\n",
      "PROGRESS: at sentence #610000, processed 9119194 words, keeping 575254 word types\n",
      "PROGRESS: at sentence #620000, processed 9274272 words, keeping 582102 word types\n",
      "PROGRESS: at sentence #630000, processed 9415220 words, keeping 587834 word types\n",
      "PROGRESS: at sentence #640000, processed 9551655 words, keeping 593382 word types\n",
      "PROGRESS: at sentence #650000, processed 9684774 words, keeping 599588 word types\n",
      "PROGRESS: at sentence #660000, processed 9826176 words, keeping 605852 word types\n",
      "PROGRESS: at sentence #670000, processed 9969118 words, keeping 611816 word types\n",
      "PROGRESS: at sentence #680000, processed 10110139 words, keeping 618061 word types\n",
      "PROGRESS: at sentence #690000, processed 10280271 words, keeping 625453 word types\n",
      "PROGRESS: at sentence #700000, processed 10438179 words, keeping 632159 word types\n",
      "PROGRESS: at sentence #710000, processed 10599895 words, keeping 638893 word types\n",
      "PROGRESS: at sentence #720000, processed 10760032 words, keeping 645644 word types\n",
      "PROGRESS: at sentence #730000, processed 10901404 words, keeping 651154 word types\n",
      "PROGRESS: at sentence #740000, processed 11056015 words, keeping 656507 word types\n",
      "PROGRESS: at sentence #750000, processed 11196573 words, keeping 663645 word types\n",
      "PROGRESS: at sentence #760000, processed 11332922 words, keeping 670917 word types\n",
      "PROGRESS: at sentence #770000, processed 11474381 words, keeping 677230 word types\n",
      "PROGRESS: at sentence #780000, processed 11619805 words, keeping 683157 word types\n",
      "PROGRESS: at sentence #790000, processed 11763426 words, keeping 688310 word types\n",
      "PROGRESS: at sentence #800000, processed 11902490 words, keeping 692021 word types\n",
      "PROGRESS: at sentence #810000, processed 12040219 words, keeping 696258 word types\n",
      "PROGRESS: at sentence #820000, processed 12197278 words, keeping 701261 word types\n",
      "PROGRESS: at sentence #830000, processed 12343849 words, keeping 707042 word types\n",
      "PROGRESS: at sentence #840000, processed 12484770 words, keeping 713488 word types\n",
      "PROGRESS: at sentence #850000, processed 12627317 words, keeping 719581 word types\n",
      "PROGRESS: at sentence #860000, processed 12788927 words, keeping 724942 word types\n",
      "PROGRESS: at sentence #870000, processed 12931102 words, keeping 729436 word types\n",
      "PROGRESS: at sentence #880000, processed 13081066 words, keeping 735391 word types\n",
      "PROGRESS: at sentence #890000, processed 13218419 words, keeping 740803 word types\n",
      "PROGRESS: at sentence #900000, processed 13373221 words, keeping 747013 word types\n",
      "PROGRESS: at sentence #910000, processed 13520262 words, keeping 752025 word types\n",
      "PROGRESS: at sentence #920000, processed 13645532 words, keeping 756429 word types\n",
      "PROGRESS: at sentence #930000, processed 13803604 words, keeping 760507 word types\n",
      "PROGRESS: at sentence #940000, processed 13965190 words, keeping 764619 word types\n",
      "PROGRESS: at sentence #950000, processed 14092397 words, keeping 768403 word types\n",
      "PROGRESS: at sentence #960000, processed 14234763 words, keeping 772230 word types\n",
      "PROGRESS: at sentence #970000, processed 14376927 words, keeping 775539 word types\n",
      "PROGRESS: at sentence #980000, processed 14513460 words, keeping 780259 word types\n",
      "PROGRESS: at sentence #990000, processed 14666621 words, keeping 785857 word types\n",
      "collected 791881 word types from a corpus of 14819198 raw words and 1000000 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 116056 unique words (14.66% of original 791881, drops 675825)', 'datetime': '2023-11-17T10:13:16.614276', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 13891348 word corpus (93.74% of original 14819198, drops 927850)', 'datetime': '2023-11-17T10:13:16.614773', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 791881 items\n",
      "sample=0.001 downsamples 46 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 11375702.624232965 word corpus (81.9%% of prior 13891348)', 'datetime': '2023-11-17T10:13:16.946996', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'prepare_vocab'}\n",
      "constructing a huffman tree from 116056 words\n",
      "built huffman tree with maximum node depth 21\n",
      "estimated required memory for 116056 words and 200 dimensions: 266928800 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-17T10:13:19.117701', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 12 workers on 116056 vocabulary and 200 features, using sg=1 hs=1 sample=0.001 negative=0 window=5 shrink_windows=True', 'datetime': '2023-11-17T10:13:19.118425', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'train'}\n",
      "EPOCH 0 - PROGRESS: at 16.42% examples, 64012 words/s, in_qsize -1, out_qsize 1\n",
      "EPOCH 0: training on 14893872 raw words (11429773 effective words) took 15.0s, 760541 effective words/s\n",
      "EPOCH 1 - PROGRESS: at 16.42% examples, 63671 words/s, in_qsize -1, out_qsize 1\n",
      "EPOCH 1: training on 14893872 raw words (11431399 effective words) took 15.1s, 758944 effective words/s\n",
      "EPOCH 2 - PROGRESS: at 16.42% examples, 63761 words/s, in_qsize -1, out_qsize 1\n",
      "EPOCH 2: training on 14893872 raw words (11430146 effective words) took 15.1s, 758156 effective words/s\n",
      "EPOCH 3 - PROGRESS: at 16.42% examples, 63260 words/s, in_qsize -1, out_qsize 1\n",
      "EPOCH 3: training on 14893872 raw words (11430148 effective words) took 15.2s, 752137 effective words/s\n",
      "EPOCH 4 - PROGRESS: at 16.66% examples, 63732 words/s, in_qsize -1, out_qsize 1\n",
      "EPOCH 4: training on 14893872 raw words (11430690 effective words) took 15.1s, 756898 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 74469360 raw words (57152156 effective words) took 76.1s, 750876 effective words/s', 'datetime': '2023-11-17T10:14:35.232895', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=116056, vector_size=200, alpha=0.025>', 'datetime': '2023-11-17T10:14:35.233351', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model_CH_1M = gensim.models.Word2Vec(corpus_file=\"./deu-ch_web-public_2019_1M-sentences_cleaned.txt\", vector_size=200, sg=1, negative=0, hs=1, workers=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c36aa-b06d-4ef0-b31b-15bad9d9b5ec",
   "metadata": {},
   "source": [
    "# Modell explorieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfebb1a8-76cf-4c7e-a06c-6a7a04783265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48618814"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CH_1M.wv.similarity(\"Chur\", \"Bern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f27614d-d435-4a2e-abf9-68e249a4b5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Winterthur', 0.6438341736793518),\n",
       " ('Biel', 0.6422432065010071),\n",
       " ('ZiegelbrÃ¼cke', 0.6350412368774414),\n",
       " ('Wetzikon', 0.602306067943573),\n",
       " ('Luzern', 0.5945485830307007),\n",
       " ('Frauenfeld', 0.5933466553688049),\n",
       " ('Olten', 0.5877766609191895),\n",
       " ('Aarau', 0.5850187540054321),\n",
       " ('Rapperswil', 0.5822216272354126),\n",
       " ('Thun', 0.580647885799408)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CH_1M.wv.most_similar(\"Chur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a264b3f-ee3d-4a9b-a69a-b329a1b5bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Snowboard', 0.6026419401168823),\n",
       " ('Sportklettern', 0.5674766302108765),\n",
       " ('Bike', 0.559036374092102),\n",
       " ('Eisklettern', 0.5579013228416443),\n",
       " ('Mountainbike', 0.5206840634346008),\n",
       " ('Kanu', 0.5193362832069397),\n",
       " ('Wintersportler', 0.5146870017051697),\n",
       " ('Langlaufen', 0.5100985169410706),\n",
       " ('Pisten', 0.5057719945907593),\n",
       " ('Famigros', 0.5052291750907898)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CH_1M.wv.most_similar(\"Ski\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34d36de7-44fb-46ef-b1a2-32148a6c30e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Servette', 0.5523718595504761),\n",
       " ('FC', 0.5478134155273438),\n",
       " ('Xamax', 0.5465849041938782),\n",
       " ('Sanogo', 0.5380123853683472),\n",
       " ('Heimspiel', 0.5341880917549133),\n",
       " ('Grasshoppers', 0.5186067819595337),\n",
       " ('Bavois', 0.5139013528823853),\n",
       " ('Supercup', 0.5125505328178406),\n",
       " ('Basket', 0.5030649304389954),\n",
       " ('Gelbschwarzen', 0.5026810765266418)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CH_1M.wv.most_similar(positive=[\"YB\", \"Eishockey\"], negative=[\"Fussball\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
