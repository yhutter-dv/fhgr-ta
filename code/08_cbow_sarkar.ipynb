{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d025a1c2df4336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T16:57:23.567435400Z",
     "start_time": "2023-11-02T16:57:23.543255200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yhutter\\AppData\\Local\\Temp\\ipykernel_30368\\2883979264.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed5d24d56d3b9d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T16:57:26.745108100Z",
     "start_time": "2023-11-02T16:57:26.727374700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740e4d077136b05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Load up sample corpus - Bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9354507a3ebaff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T16:57:33.542146300Z",
     "start_time": "2023-11-02T16:57:28.911508900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 30103\n",
      "\n",
      "Sample line: ['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
      "\n",
      "Processed line: god said let firmament midst waters let divide waters waters\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "\n",
    "bible = gutenberg.sents('bible-kjv.txt')\n",
    "remove_terms = punctuation + '0123456789'\n",
    "\n",
    "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
    "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
    "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
    "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
    "\n",
    "print('Total lines:', len(bible))\n",
    "print('\\nSample line:', bible[10])\n",
    "print('\\nProcessed line:', norm_bible[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff1a3d1e6b52bed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T16:59:11.699126800Z",
     "start_time": "2023-11-02T16:59:11.667697Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# wir kürzen \"etwas\", damit die Berechnung schneller geht...\n",
    "#  gerne später rückgängig machen\n",
    "\n",
    "norm_bible = norm_bible[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc9b730dec9cb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Implementing a word2vec model using a CBOW (Continuous Bag of Words) neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2059328232b0629",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60804df45d14a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T16:59:16.142738300Z",
     "start_time": "2023-11-02T16:59:15.983258Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras import utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b740c404e485ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Build (context_words, target_word) pair generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46201f9b3560fa4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T16:59:21.269164100Z",
     "start_time": "2023-11-02T16:59:21.176150700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            context_words.append([words[i]\n",
    "                                  for i in range(start, end)\n",
    "                                  if 0 <= i < sentence_length\n",
    "                                  and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "\n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e2dc4b0ddd440c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Build CBOW Deep Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11d19c11b6efc812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T16:59:25.263471700Z",
     "start_time": "2023-11-02T16:59:25.177422500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 4, 100)            462200    \n",
      "                                                                 \n",
      " lambda_2 (Lambda)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4622)              466822    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 929022 (3.54 MB)\n",
      "Trainable params: 929022 (3.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size * 2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eec9624ba97c87",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Train model for 5 epochs\n",
    "\n",
    "Achtung: dauert etwas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2c6d9bbbefbe584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T18:38:19.267259400Z",
     "start_time": "2023-11-02T16:59:33.673619300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 573281.9398591736\n",
      "Epoch: 2 \tLoss: 573792.0676976203\n",
      "Epoch: 3 \tLoss: 574732.1212097561\n",
      "Epoch: 4 \tLoss: 573669.8267420736\n",
      "Epoch: 5 \tLoss: 572883.4556642018\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896301a5dce61622",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Get word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b4855405ef20f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T18:38:27.158833300Z",
     "start_time": "2023-11-02T18:38:27.104151300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4621, 100)\n"
     ]
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b67f273c1691df32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T18:38:29.975241400Z",
     "start_time": "2023-11-02T18:38:29.697313400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unto</th>\n",
       "      <td>1.233148</td>\n",
       "      <td>0.716380</td>\n",
       "      <td>0.795285</td>\n",
       "      <td>0.687711</td>\n",
       "      <td>0.717798</td>\n",
       "      <td>-0.608559</td>\n",
       "      <td>0.849003</td>\n",
       "      <td>1.028589</td>\n",
       "      <td>0.206659</td>\n",
       "      <td>-0.528851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.981629</td>\n",
       "      <td>0.626677</td>\n",
       "      <td>-1.185195</td>\n",
       "      <td>-0.984692</td>\n",
       "      <td>0.810447</td>\n",
       "      <td>-1.052438</td>\n",
       "      <td>-0.872827</td>\n",
       "      <td>-0.956356</td>\n",
       "      <td>1.360290</td>\n",
       "      <td>-1.319813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lord</th>\n",
       "      <td>0.730807</td>\n",
       "      <td>-0.523129</td>\n",
       "      <td>-0.187607</td>\n",
       "      <td>0.017499</td>\n",
       "      <td>1.060174</td>\n",
       "      <td>-0.403305</td>\n",
       "      <td>-0.145262</td>\n",
       "      <td>0.806336</td>\n",
       "      <td>0.480354</td>\n",
       "      <td>0.532435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106211</td>\n",
       "      <td>0.620304</td>\n",
       "      <td>0.531243</td>\n",
       "      <td>0.392222</td>\n",
       "      <td>-0.090168</td>\n",
       "      <td>-0.739844</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>0.156161</td>\n",
       "      <td>-0.408205</td>\n",
       "      <td>0.901727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>-0.270326</td>\n",
       "      <td>-0.285472</td>\n",
       "      <td>-0.041295</td>\n",
       "      <td>-0.023545</td>\n",
       "      <td>0.403086</td>\n",
       "      <td>-0.002234</td>\n",
       "      <td>-0.364126</td>\n",
       "      <td>0.373829</td>\n",
       "      <td>0.698925</td>\n",
       "      <td>0.686178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388882</td>\n",
       "      <td>0.675534</td>\n",
       "      <td>0.415061</td>\n",
       "      <td>0.413768</td>\n",
       "      <td>0.820637</td>\n",
       "      <td>0.304831</td>\n",
       "      <td>-0.860124</td>\n",
       "      <td>0.312828</td>\n",
       "      <td>-0.102423</td>\n",
       "      <td>-1.264907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>1.529649</td>\n",
       "      <td>1.392774</td>\n",
       "      <td>0.077602</td>\n",
       "      <td>1.194607</td>\n",
       "      <td>0.088102</td>\n",
       "      <td>1.163991</td>\n",
       "      <td>-0.041349</td>\n",
       "      <td>0.251497</td>\n",
       "      <td>-1.067705</td>\n",
       "      <td>-0.505953</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.301913</td>\n",
       "      <td>1.258092</td>\n",
       "      <td>0.799970</td>\n",
       "      <td>-1.415297</td>\n",
       "      <td>-0.405153</td>\n",
       "      <td>-1.372715</td>\n",
       "      <td>-0.580238</td>\n",
       "      <td>1.005748</td>\n",
       "      <td>1.309335</td>\n",
       "      <td>-0.701609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thee</th>\n",
       "      <td>-0.047566</td>\n",
       "      <td>0.246222</td>\n",
       "      <td>-0.100607</td>\n",
       "      <td>0.052973</td>\n",
       "      <td>0.710275</td>\n",
       "      <td>0.398170</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.238995</td>\n",
       "      <td>0.174828</td>\n",
       "      <td>-0.272554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.434646</td>\n",
       "      <td>0.322128</td>\n",
       "      <td>-0.369394</td>\n",
       "      <td>-0.326891</td>\n",
       "      <td>0.548898</td>\n",
       "      <td>-0.304507</td>\n",
       "      <td>0.124086</td>\n",
       "      <td>-0.547234</td>\n",
       "      <td>0.494080</td>\n",
       "      <td>-0.517652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "unto  1.233148  0.716380  0.795285  0.687711  0.717798 -0.608559  0.849003   \n",
       "lord  0.730807 -0.523129 -0.187607  0.017499  1.060174 -0.403305 -0.145262   \n",
       "thou -0.270326 -0.285472 -0.041295 -0.023545  0.403086 -0.002234 -0.364126   \n",
       "thy   1.529649  1.392774  0.077602  1.194607  0.088102  1.163991 -0.041349   \n",
       "thee -0.047566  0.246222 -0.100607  0.052973  0.710275  0.398170  0.379861   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "unto  1.028589  0.206659 -0.528851  ... -0.981629  0.626677 -1.185195   \n",
       "lord  0.806336  0.480354  0.532435  ...  0.106211  0.620304  0.531243   \n",
       "thou  0.373829  0.698925  0.686178  ...  0.388882  0.675534  0.415061   \n",
       "thy   0.251497 -1.067705 -0.505953  ... -1.301913  1.258092  0.799970   \n",
       "thee  0.238995  0.174828 -0.272554  ... -0.434646  0.322128 -0.369394   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "unto -0.984692  0.810447 -1.052438 -0.872827 -0.956356  1.360290 -1.319813  \n",
       "lord  0.392222 -0.090168 -0.739844  0.007493  0.156161 -0.408205  0.901727  \n",
       "thou  0.413768  0.820637  0.304831 -0.860124  0.312828 -0.102423 -1.264907  \n",
       "thy  -1.415297 -0.405153 -1.372715 -0.580238  1.005748  1.309335 -0.701609  \n",
       "thee -0.326891  0.548898 -0.304507  0.124086 -0.547234  0.494080 -0.517652  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd2073602a28e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Build a distance matrix to view the most similar words (contextually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1541385adb5ce26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T18:38:37.365092700Z",
     "start_time": "2023-11-02T18:38:37.008549Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4621, 4621)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T18:39:17.474719300Z",
     "start_time": "2023-11-02T18:39:17.437994300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'god': ['hath', 'face', 'blessed', 'taken', 'behold'],\n",
       " 'noah': ['shem', 'flood', 'lamech', 'ham', 'sixty'],\n",
       " 'egypt': ['canaan', 'pharaoh', 'joseph', 'servants', 'dwell'],\n",
       " 'water': ['clothes', 'bathe', 'flesh', 'clean', 'unclean'],\n",
       " 'moses': ['aaron', 'daughter', 'pharaoh', 'balaam', 'joseph'],\n",
       " 'famine': ['phallu', 'prison', 'elder', 'shaken', 'fourscore']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term] - 1].argsort()[1:6] + 1]\n",
    "                 for search_term in ['god', 'noah', 'egypt', 'water', 'moses', 'famine']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb16b2c6639039",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
