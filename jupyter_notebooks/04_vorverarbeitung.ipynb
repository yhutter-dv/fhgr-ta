{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f99ff81-1106-4dd6-a685-3d10356d512f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     /home/yhutter/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/europarl_raw.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('gutenberg')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('europarl_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d3362521d21f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Texte vorverarbeiten\n",
    "\n",
    "einlesen, umformen, filtern, ...\n",
    "\n",
    "Nach Sarkar, angepasst von Heiko Rölke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21bb0a8e9d91d3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Einlesen von Texten aus dem Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b84f9802f7476b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'gin-top: 0;\\r\\n    margin-bottom: 0;\\r\\n}\\r\\n#pg-header #pg-machine-header strong {\\r\\n    font-weight: normal;\\r\\n}\\r\\n#pg-header #pg-start-separator, #pg-footer #pg-end-separator {\\r\\n    margin-bottom: 3em;\\r\\n    margin-left: 0;\\r\\n    margin-right: auto;\\r\\n    margin-top: 2em;\\r\\n    text-align: center\\r\\n}\\r\\n\\r\\n    .xhtml_center {text-align: center; display: block;}\\r\\n    .xhtml_center table {\\r\\n        display: table;\\r\\n        text-align: left;\\r\\n        margin-left: auto;\\r\\n        margin-right: auto;\\r\\n        }</style><title>The Project Gutenberg eBook of The Bible, King James version, Book 1: Genesis, by Anonymous</title><style>/* ************************************************************************\\r\\n * classless css copied from https://www.pgdp.net/wiki/CSS_Cookbook/Styles\\r\\n * ********************************************************************** */\\r\\n/* ************************************************************************\\r\\n * set the body margins to allow whitespace along sides of window\\r\\n * ******************************************'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
    "content = data.content\n",
    "print(content[1163:2200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd20396e41ed58",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "form, and void; and darkness was\n",
      "           upon the face of the deep. And the Spirit of God moved upon\n",
      "           the face of the waters.\n",
      "01:001:003 And God said, Let there be light: and there was light.\n",
      "01:001:004 And God saw the light, that it was good: and God divided the\n",
      "           light from the darkness.\n",
      "01:001:005 And God called the light Day, and the darkness he called\n",
      "           Night. And the evening and the morning were the first day.\n",
      "01:001:006 And God said, Let there be a firmament in the midst of the\n",
      "           waters, and let it divide the waters from the waters.\n",
      "01:001:007 And God made the firmament, and divided the waters which were\n",
      "           under the firmament from the waters which were above the\n",
      "           firmament: and it was so.\n",
      "01:001:008 And God called the firmament Heaven. And the evening and the\n",
      "           morning were the second day.\n",
      "01:001\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[1163:2045])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da09c65c43c96f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Aufteilung/Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f074da2a03551",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Satzaufteilung/Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8201858e26c70348",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "# loading text corpora\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd3e9acf6d3e0ba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144395"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total characters in Alice in Wonderland\n",
    "len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478c243306520069",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 100 characters in the corpus\n",
    "alice[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655db7df5445b536",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 4\n",
      "Sample text sentences :-\n",
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:-\n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\"\n",
      " \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\"\n",
      " 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.'\n",
      " \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\"\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "\n",
    "print('Total sentences in sample_text:', len(sample_sentences))\n",
    "print('Sample text sentences :-')\n",
    "print(np.array(sample_sentences))\n",
    "\n",
    "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
    "print('First 5 sentences in alice:-')\n",
    "print(np.array(alice_sentences[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eece1dbc596536",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Geht auch für andere Sprachen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a8d81a645c73057",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "# Total characters in the corpus\n",
    "print(len(german_text))\n",
    "# First 100 characters in the corpus\n",
    "print(german_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786eb252f1b48ff3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# default sentence tokenizer \n",
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "\n",
    "# loading german text tokenizer into a PunktSentenceTokenizer instance  \n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad41af5fc19348c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .',\n",
       " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .',\n",
       " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .',\n",
       " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .',\n",
       " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_sentences_def[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "914bc597b534eed4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .',\n",
       " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .',\n",
       " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .',\n",
       " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .',\n",
       " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c7b0be5722d83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Satzaufteilung per RegEx\n",
    "\n",
    "hier neue Ausdrucksmittel: ?<! und ?<= \"negative and positive lookbehind\"\n",
    "probieren sie die Konstruktion zum besseren Verständnis einmal in Ruhe und kleinschrittig aus (ausserhalb der Vorlesung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29edc319cf3bf8a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
       " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
       " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
       " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(\n",
    "            pattern=SENTENCE_TOKENS_PATTERN,\n",
    "            gaps=True)  # wir suchen nach den \"Lücken\" zwischen den Sätzen\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "sample_sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106057f8e7b2c909",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Wortaufteilung / Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c29774a3498691c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
     ]
    }
   ],
   "source": [
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sample_text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a292ba8b581faa06",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', 'world', 's', 'most', 'powerful', 'supercomputer', 'beats', 'China', 'The', 'US', 'has', 'unveiled', 'the', 'world', 's', 'most', 'powerful', 'supercomputer', 'called', 'Summit', 'beating', 'the', 'previous', 'record', 'holder', 'China', 's', 'Sunway', 'TaihuLight', 'With', 'a', 'peak', 'performance', 'of', '200', '000', 'trillion', 'calculations', 'per', 'second', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', 'which', 'is', 'capable', 'of', '93', '000', 'trillion', 'calculations', 'per', 'second', 'Summit', 'has', '4', '608', 'servers', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts']\n"
     ]
    }
   ],
   "source": [
    "TOKEN_PATTERN = r'\\w+'        \n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,\n",
    "                                gaps=False)  # hier suchen wir nach den Wörtern\n",
    "words = regex_wt.tokenize(sample_text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "587b2e343e702356",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,', 'beats', 'China.', 'The', 'US', 'has', 'unveiled', 'the', \"world's\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit',\", 'beating', 'the', 'previous', 'record-holder', \"China's\", 'Sunway', 'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second,', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight,', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second.', 'Summit', 'has', '4,608', 'servers,', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts.']\n"
     ]
    }
   ],
   "source": [
    "GAP_PATTERN = r'\\s+'        \n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,\n",
    "                                gaps=True)  # und wieder nach den Lücken\n",
    "words = regex_wt.tokenize(sample_text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c0cf7f9660c1a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "(Sarkar führt noch andere Tokenizer vor, schauen sie bei Interesse dort nach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cbf80f8969f25d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Tokenization mit spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1db7c33c5ad5bb81",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yhutter/GitRepos/fhgr-ta/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "text_spacy = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52b7f8052389a3e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[US unveils world's most powerful supercomputer, beats China.,\n",
       " The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.,\n",
       " With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.,\n",
       " Summit has 4,608 servers, which reportedly take up the size of two tennis courts.]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = list(text_spacy.sents)\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440821d2e7de529",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wörter aufgeteilt nach Sätzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d15aac4303dd2dc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.'], ['The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating', 'the', 'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.'], ['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.'], ['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']]\n"
     ]
    }
   ],
   "source": [
    "sent_words = [[word.text for word in sent] for sent in sents]\n",
    "print(sent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308cfcea7cc0336d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "oder durchgehend in einer Liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39e90e84bb22122e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating', 'the', 'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
     ]
    }
   ],
   "source": [
    "words = [word.text for word in text_spacy]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3405c97a787bf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Akzente und Umlaute entfernen\n",
    "\n",
    "(nicht immer notwendig, aber manchmal schon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc442e94c187a657",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5f3901696d9eb14",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ausserlich, auerlich, uberhaupt, offentlich'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so wirkt es für deutsche Umlaute:\n",
    "remove_accented_chars(\"äusserlich, äußerlich, überhaupt, öffentlich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a883785bc1b0d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "(das kann man sicherlich besser hinbekommen...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec73e5ce57b4db",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Contractions / Zusammenziehungen bzw. Auslassungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95a53cb7-4561-430b-89d9-5162b4b71824",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b142ccbc2565f040",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    # HR: hier wird mit einem compilierten Pattern gearbeitet; dieses wird aus allen Contraction-Schlüsseln zusammengesetzt\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    # HR: innere Funktion\n",
    "    # HR: Achtung - die Sarkar-Funktion arbeitet nicht immer fehlerfrei. Hier (hoffentlich) korrigiert.\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        # Contraction-Muster sind alle kleingeschrieben\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())                     \n",
    "        # Grossschreibung wiederherstellen\n",
    "        if match[0].isupper():\n",
    "            expanded_contraction = expanded_contraction[0].upper() + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)  # expand match wird für jeden match aufgerufen\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)  # noch etwas aufräumen\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afd3e294968c2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "expand_contractions(\"Y'all can't expand contractions I'd think\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0858d8e8ed1b2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "expand_contractions(\"Ain't ain't good you'd think.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9bd1156ba0dc53",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Remove Special Characters / Spezialzeichen entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7713971c28c7ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "remove_special_characters(\"Well this was fun! What do you think? 123#@!\", \n",
    "                          remove_digits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf93ba69a915a4c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Text Correction / Fehlerverbesserungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc492b6a6a45793e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Zeichenwiederholungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf269aa08fdc995",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "zuerst \"naive\" Lösung: alle Zeichenwiederholungen ersetzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca983608024ef4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "old_word = 'finalllyyy'\n",
    "# neue Konstrukte RegEx: Anzahlen/Wiederholungen und Verweise auf gefundene Teil-Muster\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "step = 1\n",
    "\n",
    "while True:\n",
    "    # remove one repeated character\n",
    "    new_word = repeat_pattern.sub(match_substitution,\n",
    "                                  old_word)\n",
    "    if new_word != old_word:\n",
    "         print('Step: {} Word: {}'.format(step, new_word))\n",
    "         step += 1 # update step\n",
    "         # update old word to last substituted state\n",
    "         old_word = new_word  \n",
    "         continue\n",
    "    else:\n",
    "         print(\"Final word:\", new_word)\n",
    "         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829a44295395678",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "wie wir sehen, können so auch zu viele Zeichen entfernt werden\n",
    "\n",
    "besser: \n",
    "nur solange entfernen, bis \"richtiges\" Wort erreicht wird (Wörterbuch-Ansatz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183077daf3c1c18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "old_word = 'finalllyyy'\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "step = 1\n",
    " \n",
    "while True:\n",
    "    # check for semantically correct word\n",
    "    if wordnet.synsets(old_word):\n",
    "        print(\"Final correct word:\", old_word)\n",
    "        break\n",
    "    # remove one repeated character\n",
    "    new_word = repeat_pattern.sub(match_substitution,\n",
    "                                  old_word)\n",
    "    if new_word != old_word:\n",
    "        print('Step: {} Word: {}'.format(step, new_word))\n",
    "        step += 1 # update step\n",
    "        # update old word to last substituted state\n",
    "        old_word = new_word  \n",
    "        continue\n",
    "    else:\n",
    "        print(\"Final word:\", new_word)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da4735dcb3fbdc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "als Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a433cf61b67158c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc447056b7a4e95",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
    "correct_tokens = remove_repeated_characters(nltk.word_tokenize(sample_sentence))\n",
    "' '.join(correct_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a6345f23ca475",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "beispielsatz = (\"Hallloo? Wiiee jetzt?\")\n",
    "correct_tokens = remove_repeated_characters(nltk.word_tokenize(beispielsatz))\n",
    "' '.join(correct_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5507ff36f8e452ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "(klappt nicht gut, da falsches Wörterbuch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4eb27d7bdb4d18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Rechtschreibkorrekturen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18765119c3e5b1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def tokens(text): \n",
    "    \"\"\"\n",
    "    Get all words from the corpus\n",
    "    \"\"\"\n",
    "    return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "WORDS = tokens(open('big.txt').read())\n",
    "WORD_COUNTS = collections.Counter(WORDS)\n",
    "# top 10 words in corpus\n",
    "WORD_COUNTS.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efed00c5a856ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def edits0(word): \n",
    "    \"\"\"\n",
    "    Return all strings that are zero edits away \n",
    "    from the input word (i.e., the word itself).\n",
    "    \"\"\"\n",
    "    return {word}\n",
    "\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    Return all strings that are one edit away \n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    def splits(word):\n",
    "        \"\"\"\n",
    "        Return a list of all possible (first, rest) pairs \n",
    "        that the input word is made of.\n",
    "        \"\"\"\n",
    "        return [(word[:i], word[i:]) \n",
    "                for i in range(len(word)+1)]\n",
    "                \n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"\"\"Return all strings that are two edits away \n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7cbfd63a6a7fc9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"\"\"\n",
    "    Return the subset of words that are actually \n",
    "    in our WORD_COUNTS dictionary.\n",
    "    \"\"\"\n",
    "    return {w for w in words if w in WORD_COUNTS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd0c8dcdc2fc10",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# input word\n",
    "word = 'fianlly'\n",
    "\n",
    "# zero edit distance from input word\n",
    "edits0(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab6fced342bc04",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# returns null set since it is not a valid word\n",
    "known(edits0(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c06b93d50b204",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# one edit distance from input word\n",
    "edits1(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44cfdbb5e19aa5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get correct words from above set\n",
    "known(edits1(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0024c6f448c84c4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# two edit distances from input word\n",
    "edits2(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14497d24b67b54ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get correct words from above set\n",
    "known(edits2(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287532663a44485",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Vorgehen: ist das Wort selbst bekannt? Falls nicht: Editierdistanz 1 usw. Fallback: Das Wort selbst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89f23dfe0de376",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "candidates = (known(edits0(word)) or \n",
    "              known(edits1(word)) or \n",
    "              known(edits2(word)) or \n",
    "              [word])\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78964c899bfd77e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"\"\"\n",
    "    Get the best correct spelling for the input word\n",
    "    \"\"\"\n",
    "    # Priority is for edit distance 0, then 1, then 2\n",
    "    # else defaults to the input word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=WORD_COUNTS.get)  # falls es mehrere Möglichkeiten gibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e7d0bf2ea7b8a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "correct('fianlly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a7ce08b4bdee2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "correct('FIANLLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ac167a5670520",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def correct_match(match):\n",
    "    \"\"\"\n",
    "    Spell-correct word in match, \n",
    "    and preserve proper upper/lower/title case.\n",
    "    \"\"\"\n",
    "    \n",
    "    word = match.group()\n",
    "    def case_of(text):\n",
    "        \"\"\"\n",
    "        Return the case-function appropriate \n",
    "        for text: upper, lower, title, or just str.:\n",
    "            \"\"\"\n",
    "        return (str.upper if text.isupper() else\n",
    "                str.lower if text.islower() else\n",
    "                str.title if text.istitle() else\n",
    "                str)\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "    \n",
    "def correct_text_generic(text):\n",
    "    \"\"\"\n",
    "    Correct all the words within a text, \n",
    "    returning the corrected text.\n",
    "    \"\"\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919a95614dc3bdc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "correct_text_generic('fianlly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ed12733bcfb4b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "correct_text_generic('FIANLLY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde799b224d6b6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Rechtschreibkorrekturen in Bibliotheken (Beispiel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949915554caf84f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !pip install textblob\n",
    "from textblob import Word\n",
    "\n",
    "w = Word('fianlly')\n",
    "w.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71017611cad2e003",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf85df3a4a3b83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "w = Word('flaot')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34686500e19f0c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Stemming / Wortstamm-Reduktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1340767dbbc11c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Porter Stemmer: Regelbasiert, Anwendung fester Verkürzungsregeln in aufeinander folgenden Phasen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ba49bfc3c628",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64f9468373ea15",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ps.stem('lying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4e8a5ffbd693e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ps.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac7e46ca91fe83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Lancaster Stemmer: ebenfalls regelbasiert, aber teils andere Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959377b1e64691d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Lancaster Stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394e712fc775930",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ls.stem('lying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e52ef6b73c239",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ls.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b075baf8e5d80",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wie kann man so etwas nachbauen? Ganz simpel zum Beispiel als RegEx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d01d2b2f06406",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Regex based stemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831465dc0fc4bbc0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rs.stem('lying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc1c3a9bb4aff8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rs.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea8bae1f2412f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Stemming geht natürlich nicht nur im Englischen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7a3894a154c06",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"german\")\n",
    "print('Supported Languages:', SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d76272dc60b839",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Originalkommentare von Sarkar :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ee89c1becb62c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# stemming on German words\n",
    "# autobahnen -> cars\n",
    "# autobahn -> car\n",
    "ss.stem('autobahnen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e50bd041a5f5d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# springen -> jumping\n",
    "# spring -> jump\n",
    "ss.stem('springen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c473064a900ec67",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "weitere Beispiele (HR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6eb2a5412c3fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ss.stem(\"gesprungen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29919ead7ccf3d9e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Als Funktion für Sätze/Texte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53b032582688e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078856c8e5d20da",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Lemmatization / Lemmatisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5f64b92d5d461",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a8df1c0dc535a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('men', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d7fc17f6c3b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45035c45087fff22",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ineffective lemmatization\n",
    "print(wnl.lemmatize('ate', 'n'))\n",
    "print(wnl.lemmatize('fancier', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310d20f30ea8700",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28ad133636069b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Stoppwörter entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e08bfa09c8f4aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce26cb41abb83a4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Alles zusammenführen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb90dc9d1ff8cb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb15be15ab28b57",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "{'Original': sample_text,\n",
    " 'Processed': normalize_corpus([sample_text])[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c92436dbccf93",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
