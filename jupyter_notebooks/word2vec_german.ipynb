{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aae3868-aaaf-4068-9bbd-b40f6c20fe2e",
   "metadata": {},
   "source": [
    "# Aufgabenstellung\n",
    "Übertragen sie die Berechnung von Wortvektoren als Embeddings auf die deutsche Sprache. Verwenden sie zuerst einen sehr kleinen Korpus, auch wenn damit keine guten Ergebnisse zu erzielen sind. Es geht ums Prinzip der Berechnung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d5841-9270-437e-bd84-639604682d4d",
   "metadata": {},
   "source": [
    "# Text einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5b0071-30e8-464f-a29e-907a90a207f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb0da815-07be-4913-b618-b1f4f39001e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 318\n",
      "\n",
      "Sample line: Regula Amrain war die Frau eines abwesenden Seldwylers; dieser hatte einen großen Steinbruch hinter dem Städtchen besessen und seine Zeitlang ausgebeutet und zwar auf Seldwyler Art. Das ganze Nest war beinahe aus dem guten Sandstein gebaut, aus welchem der Berg bestand; aber das Schuldenwesen, das auf den Häusern ruhte, hatte von jeher recht eigentlich schon mit den Steinen begonnen, aus denen sie gebaut waren; denn nichts schien den Seldwylern so wohl geeignet als Stoff und Gegenstand eines muntern Verkehrs als ein solcher Steinbruch, und derselbe glich einer in Felsen gehauenen römischen Schaubühne, über welche die Besitzer emsig hinwegliefen, einer den andern jagend.\n",
      "\n",
      "\n",
      "Processed line: regula amrain frau abwesenden seldwylers groen steinbruch städtchen besessen zeitlang ausgebeutet seldwyler art ganze nest beinahe guten sandstein gebaut berg bestand schuldenwesen häusern ruhte jeher recht eigentlich schon steinen begonnen denen gebaut schien seldwylern wohl geeignet stoff gegenstand muntern verkehrs steinbruch glich felsen gehauenen römischen schaubühne besitzer emsig hinwegliefen jagend\n"
     ]
    }
   ],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    remove_terms = punctuation + '0123456789'\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-ZäöüÄÖÜ\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "text_corpus = []\n",
    "\n",
    "with open(\"./amrain.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_amrain = f.readlines()\n",
    "    \n",
    "with open(\"./pankraz.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_pankraz = f.readlines()\n",
    "\n",
    "text_corpus = text_amrain + text_pankraz\n",
    "\n",
    "remove_terms = punctuation + '0123456789'\n",
    "norm_text = [[word.lower() for word in sent if word not in remove_terms] for sent in text_corpus]\n",
    "norm_text = [''.join(sent) for sent in norm_text]\n",
    "\n",
    "\n",
    "norm_text = filter(None, normalize_corpus(norm_text))\n",
    "norm_text = [tok_sent for tok_sent in norm_text if len(tok_sent.split()) > 2]\n",
    "print('Total lines:', len(text_corpus))\n",
    "print('\\nSample line:', text_corpus[0])\n",
    "print('\\nProcessed line:', norm_text[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7768b67-384a-4e96-80da-3dc40c57d4fe",
   "metadata": {},
   "source": [
    "# Vokabular erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0d7e625-ff2a-4f76-b811-f391ffcbe3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 6909\n",
      "Vocabulary Sample: [('mehr', 1), ('mutter', 2), ('wurde', 3), ('ganz', 4), ('schon', 5), ('sah', 6), ('wohl', 7), ('ging', 8), ('sagte', 9), ('frau', 10)]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras import utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_text)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_text]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0257cb-43c2-4c99-9985-30859ada00a7",
   "metadata": {},
   "source": [
    "# Kontextbezug herstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecdd2654-62ce-4f9f-881c-00e149d37ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['regula', 'amrain', 'abwesenden', 'seldwylers'] -> Target (Y): frau\n",
      "Context (X): ['amrain', 'frau', 'seldwylers', 'groen'] -> Target (Y): abwesenden\n",
      "Context (X): ['frau', 'abwesenden', 'groen', 'steinbruch'] -> Target (Y): seldwylers\n",
      "Context (X): ['abwesenden', 'seldwylers', 'steinbruch', 'städtchen'] -> Target (Y): groen\n",
      "Context (X): ['seldwylers', 'groen', 'städtchen', 'besessen'] -> Target (Y): steinbruch\n",
      "Context (X): ['groen', 'steinbruch', 'besessen', 'zeitlang'] -> Target (Y): städtchen\n",
      "Context (X): ['steinbruch', 'städtchen', 'zeitlang', 'ausgebeutet'] -> Target (Y): besessen\n",
      "Context (X): ['städtchen', 'besessen', 'ausgebeutet', 'seldwyler'] -> Target (Y): zeitlang\n",
      "Context (X): ['besessen', 'zeitlang', 'seldwyler', 'art'] -> Target (Y): ausgebeutet\n",
      "Context (X): ['zeitlang', 'ausgebeutet', 'art', 'ganze'] -> Target (Y): seldwyler\n",
      "Context (X): ['ausgebeutet', 'seldwyler', 'ganze', 'nest'] -> Target (Y): art\n"
     ]
    }
   ],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            context_words.append([words[i]\n",
    "                                  for i in range(start, end)\n",
    "                                  if 0 <= i < sentence_length\n",
    "                                  and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "\n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302039b-2f49-4f3f-9fc9-f5ec158fb1ee",
   "metadata": {},
   "source": [
    "# Neuronales Netzwerk erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da6af71b-6e48-4c4e-a463-23ceb56cda82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 4, 100)            690900    \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6909)              697809    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1388709 (5.30 MB)\n",
      "Trainable params: 1388709 (5.30 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size * 2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3abcc-b0ea-4aa4-bb13-300c006696bd",
   "metadata": {},
   "source": [
    "# Modell für 5 Epochen trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e3eeb3d-c493-4723-85b5-922309c1a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 138485.29325437546\n",
      "\n",
      "Epoch: 2 \tLoss: 138084.50889873505\n",
      "\n",
      "Epoch: 3 \tLoss: 137780.97097921371\n",
      "\n",
      "Epoch: 4 \tLoss: 137383.44759082794\n",
      "\n",
      "Epoch: 5 \tLoss: 136825.6836619377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae196a-9d60-465e-9b82-119413a15853",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e53dc99-47cf-483d-9bb8-8b047d732c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6908, 100)\n"
     ]
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ee414fa-70ce-444e-98a2-5f3367f8cdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mutter</th>\n",
       "      <td>0.900185</td>\n",
       "      <td>0.436805</td>\n",
       "      <td>-0.590087</td>\n",
       "      <td>0.424985</td>\n",
       "      <td>0.191628</td>\n",
       "      <td>0.309301</td>\n",
       "      <td>-0.219675</td>\n",
       "      <td>0.216454</td>\n",
       "      <td>-0.657697</td>\n",
       "      <td>0.710062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486133</td>\n",
       "      <td>0.118618</td>\n",
       "      <td>0.477306</td>\n",
       "      <td>0.223108</td>\n",
       "      <td>0.535116</td>\n",
       "      <td>-0.388917</td>\n",
       "      <td>0.377261</td>\n",
       "      <td>-0.454641</td>\n",
       "      <td>-1.067472</td>\n",
       "      <td>-0.445360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wurde</th>\n",
       "      <td>0.917882</td>\n",
       "      <td>0.055035</td>\n",
       "      <td>-0.232964</td>\n",
       "      <td>0.447332</td>\n",
       "      <td>-0.280784</td>\n",
       "      <td>0.824357</td>\n",
       "      <td>-0.641831</td>\n",
       "      <td>0.539870</td>\n",
       "      <td>-0.850077</td>\n",
       "      <td>0.359259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506576</td>\n",
       "      <td>-0.200272</td>\n",
       "      <td>-0.078323</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.512242</td>\n",
       "      <td>0.454328</td>\n",
       "      <td>1.032714</td>\n",
       "      <td>0.355244</td>\n",
       "      <td>-0.513173</td>\n",
       "      <td>0.344595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ganz</th>\n",
       "      <td>0.468482</td>\n",
       "      <td>0.695844</td>\n",
       "      <td>-0.121286</td>\n",
       "      <td>0.839853</td>\n",
       "      <td>0.423197</td>\n",
       "      <td>-0.353781</td>\n",
       "      <td>0.380722</td>\n",
       "      <td>0.449927</td>\n",
       "      <td>-0.488304</td>\n",
       "      <td>0.412985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084821</td>\n",
       "      <td>-0.057305</td>\n",
       "      <td>0.495286</td>\n",
       "      <td>-0.174868</td>\n",
       "      <td>0.143616</td>\n",
       "      <td>-0.109467</td>\n",
       "      <td>-0.104221</td>\n",
       "      <td>-0.095787</td>\n",
       "      <td>0.075746</td>\n",
       "      <td>0.171558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schon</th>\n",
       "      <td>0.603444</td>\n",
       "      <td>0.301495</td>\n",
       "      <td>0.435678</td>\n",
       "      <td>0.584432</td>\n",
       "      <td>-0.219162</td>\n",
       "      <td>-0.321656</td>\n",
       "      <td>0.360808</td>\n",
       "      <td>0.581558</td>\n",
       "      <td>-0.058781</td>\n",
       "      <td>0.406662</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180848</td>\n",
       "      <td>-0.163308</td>\n",
       "      <td>0.084813</td>\n",
       "      <td>0.575552</td>\n",
       "      <td>0.446059</td>\n",
       "      <td>-0.074478</td>\n",
       "      <td>-0.434575</td>\n",
       "      <td>-0.251001</td>\n",
       "      <td>-0.755862</td>\n",
       "      <td>-0.622512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sah</th>\n",
       "      <td>0.642320</td>\n",
       "      <td>-0.585364</td>\n",
       "      <td>0.461161</td>\n",
       "      <td>-0.645764</td>\n",
       "      <td>0.501583</td>\n",
       "      <td>-0.560788</td>\n",
       "      <td>-0.104652</td>\n",
       "      <td>0.420738</td>\n",
       "      <td>0.223776</td>\n",
       "      <td>-0.286944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808237</td>\n",
       "      <td>-0.149937</td>\n",
       "      <td>-0.142692</td>\n",
       "      <td>-0.296029</td>\n",
       "      <td>0.487739</td>\n",
       "      <td>0.114529</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>0.034719</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>-0.319899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "mutter  0.900185  0.436805 -0.590087  0.424985  0.191628  0.309301 -0.219675   \n",
       "wurde   0.917882  0.055035 -0.232964  0.447332 -0.280784  0.824357 -0.641831   \n",
       "ganz    0.468482  0.695844 -0.121286  0.839853  0.423197 -0.353781  0.380722   \n",
       "schon   0.603444  0.301495  0.435678  0.584432 -0.219162 -0.321656  0.360808   \n",
       "sah     0.642320 -0.585364  0.461161 -0.645764  0.501583 -0.560788 -0.104652   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "mutter  0.216454 -0.657697  0.710062  ...  0.486133  0.118618  0.477306   \n",
       "wurde   0.539870 -0.850077  0.359259  ...  0.506576 -0.200272 -0.078323   \n",
       "ganz    0.449927 -0.488304  0.412985  ... -0.084821 -0.057305  0.495286   \n",
       "schon   0.581558 -0.058781  0.406662  ... -0.180848 -0.163308  0.084813   \n",
       "sah     0.420738  0.223776 -0.286944  ...  0.808237 -0.149937 -0.142692   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "mutter  0.223108  0.535116 -0.388917  0.377261 -0.454641 -1.067472 -0.445360  \n",
       "wurde   0.374256  0.512242  0.454328  1.032714  0.355244 -0.513173  0.344595  \n",
       "ganz   -0.174868  0.143616 -0.109467 -0.104221 -0.095787  0.075746  0.171558  \n",
       "schon   0.575552  0.446059 -0.074478 -0.434575 -0.251001 -0.755862 -0.622512  \n",
       "sah    -0.296029  0.487739  0.114529  0.867700  0.034719  0.251700 -0.319899  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54945eb-e135-4199-a5b7-00daf6c8b473",
   "metadata": {},
   "source": [
    "# Distanzmatrix bilden und ähnliche Wörter finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "387c7ea5-2590-4721-85ff-021cf0ebb71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6908, 6908)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d6af205-f197-49db-8b50-f07af0cb974a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amrain': ['regula', 'regel', 'nachdem', 'etwa', 'daher'],\n",
       " 'städtchen': ['nachzufahren', 'ganzes', 'tatzen', 'wüster', 'begibt'],\n",
       " 'ausgebeutet': ['handlichen',\n",
       "  'republikaner',\n",
       "  'kleinlichsten',\n",
       "  'klammernd',\n",
       "  'einrichtungen'],\n",
       " 'zeitlang': ['schleunigst',\n",
       "  'zärtlichkeit',\n",
       "  'anblick',\n",
       "  'gebaut',\n",
       "  'anzukünden'],\n",
       " 'abwesenden': ['suchten',\n",
       "  'scheidung',\n",
       "  'vierzehnten',\n",
       "  'beschaffenheit',\n",
       "  'nahegelegt'],\n",
       " 'regula': ['regel', 'nachdem', 'deswegen', 'steckte', 'wohlbegütert']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term] - 1].argsort()[1:6] + 1]\n",
    "                 for search_term in ['amrain', 'städtchen', 'ausgebeutet', 'zeitlang', 'abwesenden', 'regula']}\n",
    "\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
